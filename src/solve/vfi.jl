@doc raw"""The Bellman equation"""
function bellman_value(model::AiyagariDiscrete, v_guess, uâ‚œ)
    @unpack r, w, aÌ², Î², Î , n, m, a_grid, l_grid = model
    # Expected value in the next period
    @inbounds  ð”¼v = Î ' * v_guess' |> v -> reshape(repeat(v, inner=(n,1)), n, m, n)
    # Compute the value function over all posible states
    vâ‚œ = uâ‚œ + Î²*ð”¼v
    # Find the optimal desision for each of the current states
    optimal_decision = map(findmax, eachslice(vâ‚œ, dims=(1,2), drop=true))
    v_iterated = first.(optimal_decision)
    policy_indices = last.(optimal_decision)
    error = abs.(v_guess .- v_iterated)
    # return stuffs
    return v_iterated, error, policy_indices
end


@doc raw"""Value function iteration"""
function value_function_iterate(model::AiyagariDiscrete; max_iter=1e5, tol=1e-7)
    @unpack r, w, aÌ², Î², Î , a_grid, l_grid, v_initial, l_stationary_dist, n, m = model
    i = 1
    # Caching the first term in the bellman equation: consumption utility
    câ‚œ = [(1 + r) * aâ‚œ +  w * l - aâ‚œâ‚Šâ‚ for aâ‚œ in a_grid, l in l_grid, aâ‚œâ‚Šâ‚ in a_grid]
    uâ‚œ = LogUtility().(câ‚œ)
    # Starts the iteration process
    v_fn, error, policy_indices = bellman_value(model, v_initial, uâ‚œ)
    while any(error .>= tol) && i <= max_iter
        v_fn, error, policy_indices = bellman_value(model, v_fn, uâ‚œ)
        i += 1
    end
    # Get the optimal policy
    policy = a_grid[policy_indices]
    # capital distribution
    a_transition_matrix = zeros(n, n, m)
    for l_idx in 1:m
        for a_idx in 1:n
            a_transition_matrix[a_idx, policy_indices[a_idx, l_idx], l_idx] = 1
        end
    end
    println("Value-function iteration terminated after " * string(i) * " iterations.")
    return AiyagariDiscreteSolution(v_fn, policy, a_transition_matrix)
end
